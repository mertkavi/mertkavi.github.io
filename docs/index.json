[{"content":" Learn more ","date":null,"permalink":"/","section":"","summary":" Learn more ","title":""},{"content":"","date":null,"permalink":"/tags/artificial-intelligence/","section":"Tags","summary":"","title":"artificial intelligence"},{"content":"","date":null,"permalink":"/tags/large-language-model/","section":"Tags","summary":"","title":"large language model"},{"content":"","date":null,"permalink":"/tags/rag/","section":"Tags","summary":"","title":"rag"},{"content":" Disclaimer: Any opinions expressed are solely my own. Introduction #Numerous large language models are constantly being introduced today. Notable ones like IBMâ€™s Granite1, LLama 2, FLAN-T5 3, and Alpaca 4 stand out due to their unique sizes. These models perform differently on various tests like MMLU, TriviaQA, GSM8K, and others.\nThis document explores a promising solution to these challenges, known as the RAG approach. This augmentation not only elevates the accuracy of responses but also amplifies their overall effectiveness.\nThe proposed system offers a range of key benefits, including continuous and incremental responsiveness, adaptation to new contextual information, and a reduction in data dependencies. In the following sections, we delve into the components and flow of this innovative approach, shedding light on how it addresses the challenges posed by large language models.\nProblem #The rise of large language models brings about various difficulties:\nModels rely on static data and lack updates with current information. Despite their general-purpose design, they lack domain-specific knowledge. The process of training and generating these models incurs substantial costs. Concerns about privacy arise due to the absence of safeguards for private data. These concerns make the adoption of general-purpose large language models increasingly complex in corporate environments.\nProposed System #The Retrieval Augmented Generation (RAG) approach is one of the most effective solutions for addressing these issues. Think of RAG as a medicinal injection administered to the foundational model, enhancing its response in conjunction with a Language Model (LM). This augmentation not only improves response accuracy but also amplifies its effectiveness.\nRAG seamlessly integrates with the capabilities of semantic search, drawing inspiration from the field of Information Retrieval 5, and harnesses the power of artificial intelligence. These capabilities are harnessed within the context of trained data on the foundational model, as well as incorporating enterprise-specific and domain-specific data. The information retrieval aspect can be facilitated through technologies like Full-Text Search or Vector Database, which seamlessly communicate with the foundational model.\nWhen your foundational model is trained solely on static data, it lacks the ability to adapt to new data and respond to evolving content. While it is possible to retrain the model to fresh data, this process can be cost-prohibitive.\nThe proposed system encompasses the following key procedures:\nContinuous and incremental responsiveness. Adaptation to new contextual information. Reduction of data dependencies. Components # High-Level Retrieval Augmented Generation Architecture Data Lake #The data lake serves as a repository housing a diverse range of documents, including domain-specific documentation, manuals, posts, and spreadsheets. These documents contain valuable information related to your enterprise\u0026rsquo;s memory and product details, which are essential for your business operations.\nThis data is prepared for integration into our semantic search system.\nSearch Engine #Our search engine can be powered by Full-Text Search, Vector Database or another compatible system. These engines offer advanced capabilities tailored to your specific requirements, such as result ranking based on input search queries. The search engine is utilized by our Foundational Model to enhance and augment search results effectively.\nFoundational Model #The foundational model can be selected from among the models currently available, such as IBMâ€™s Granite1, LLama 2 or FLAN-T5 3. I won\u0026rsquo;t delve into the concept of the Foundational Model 6 in this post.\nLLM Application #The LLM application functions as a front-end and back-end package, accepting Questions or Queries from external sources, initiating actions, and forwarding the generated responses to other systems, such as an API or any user interface.\nWorkflow #In the Workflow section, the proposed RAG architecture is described in two main parts:\nPart 1: Data Preparation and Indexing # Select, clean, and consolidate necessary documents or tables from the data lake. Automate the first point using a workflow management tool such as Airflow. Index the data into Full-Text Search Database or create embedding model to Vector Database such as Milvus 7. Package and create deployable objects for the third point due to the need for scheduling and fail-over nodes in the workflow. Apply fine-tuning principles to the indexing process to gather the best-qualified ranking results from Full-Text Search or Milvus 7. I assume there is a deployed foundational model in a production environment.\nPart 2: Contextual Response Generation # Receive a question from the LLM Application. Execute a query on Full-Text Search Database or Milvus 7. and retrieve highly ranked results. Concatenate the question and the results to form a context and pass it to the foundational model. Take the generated response and deliver it to listener objects. This structured flow enables the RAG architecture to seamlessly integrate information retrieval and language generation, addressing the challenges posed by large language models.\nConclusion #There are opportunities for improvements and enhancements to the RAG architecture, which I would like to address in this section.\nBERT Re-ranker #You can apply advanced re-ranking strategies and neural engines to enhance the Search Engine component, including models like ColBERT 8, which is a retrieval model enabling BERT-based search over large text collections.\nFine Tuning #Fine-tuning is crucial not only for language models but for any machine learning model. There are various approaches in the literature, such as QLoRA 9.\nReinforcement Learning #There are various methods for applying Reinforcement Learning to generate feedback-looped LLM. These models can work with human feedback via outputs and human evaluators 10.\nExample enhancements can support your LLM Application in becoming more accurate, stable, and less prone to hallucinations.\nThe Retrieval Augmented Generation (RAG) approach represents a groundbreaking synergy between information retrieval and language generation, offering a promising solution to the challenges of large language models.\nIBMâ€™s Granite\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLLama\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFLAN-T5\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAlpaca\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nInformation Retrieval\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFoundational Models\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMilvus\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nColBERT\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nQLoRA\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nRLHF\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"6 October 2023","permalink":"/rag-architecture-empowering-large-language-models/","section":"Writings","summary":"Disclaimer: Any opinions expressed are solely my own.","title":"RAG Architecture: Empowering Large Language Models"},{"content":"","date":null,"permalink":"/tags/retrieval-augmented-generation/","section":"Tags","summary":"","title":"retrieval augmented generation"},{"content":"","date":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags"},{"content":" I write my thoughts here. ","date":null,"permalink":"/posts/","section":"Writings","summary":"I write my thoughts here.","title":"Writings"},{"content":"","date":null,"permalink":"/tags/data-engineering/","section":"Tags","summary":"","title":"data engineering"},{"content":"","date":null,"permalink":"/tags/duckdb/","section":"Tags","summary":"","title":"duckdb"},{"content":"","date":null,"permalink":"/tags/polars/","section":"Tags","summary":"","title":"polars"},{"content":"","date":null,"permalink":"/tags/spark/","section":"Tags","summary":"","title":"spark"},{"content":" Disclaimer: Any opinions expressed are solely my own. Introduction #There are many data processing technologies available today compared to the past. Some of them work as distributed systems, while others work as standalone solutions. There is no silver bullet, as all of these technologies cover specialized problems. In this post, we will discuss a basic data processing job that can be performed using Spark, DuckDB, and Polars.\nProblem #The goal of the basic data processing job is to perform the following steps:\nRead a JSON file. Write a Parquet file. Thus, the job involves converting a JSON file to a Parquet file. Naturally, our data is complex, unpredictable, and heavily contaminated, just like real-world data.\nProposed System #DuckDB #CREATE TABLE d AS SELECT * FROM \u0026#39;d.json\u0026#39;; COPY d to \u0026#39;d.parquet\u0026#39; (FORMAT parquet); Unfortunately, this process takes more than 30 minutes ðŸ¤·.\nPolars #import polars as pl df = pl.read_ndjson(\u0026#34;d.json\u0026#34;) 1028 source = normalise_filepath(source) 1030 self = cls.__new__(cls) -\u0026gt; 1031 self._df = PyDataFrame.read_ndjson(source) 1032 return self RuntimeError: BindingsError: \u0026#34;expected list/array in JSON value, got str\u0026#34; Once again, there are strange errors for very basic commands.\nSpark #./bin/spark-shell --master \u0026#34;local[8]\u0026#34; val df = spark.read.json(path) df.coalesce(1).write.parquet(outputPath) The task completes within a few minutes.\nConclusion #I needed to perform some basic data processing jobs quickly, but unfortunately, I faced some obstacles\u0026hellip; I\u0026rsquo;m not sure, maybe DuckDB needs some optimizations for complex data structures, and Polars requires some more time. This short story shows me that no matter what, Spark still just works if you have the necessary skills.\n","date":"17 June 2023","permalink":"/spark-is-still-a-safe-port-when-compared-to-duckdb-and-polars/","section":"Writings","summary":"Disclaimer: Any opinions expressed are solely my own.","title":"Spark is still a safe port when compared to DuckDB and Polars"},{"content":" Disclaimer: Any opinions expressed are solely my own. Introduction #In today\u0026rsquo;s world, nearly every product can be considered a data product. However, the ways in which they use data can vary greatly, with some being more data-intensive than others. This presents new challenges for data-reliant systems, such as ensuring data freshness and providing quick responses to queries. Businesses need to be able to gain insights from this data in a timely manner to stay competitive. Furthermore, domain data has become increasingly complex, with numerous data types like maps, tuples, arrays, and structs.\nThis complexity makes it difficult to gather data through queries due to the growing number of processing stages, time series, graphs, and business logic involved.\nProblem #The system for processing and analyzing data is constantly evolving, and managing it has become increasingly difficult as the amount of data being collected continues to increase. With data platforms and infrastructure advancing at a rapid pace, duplicate components and performance issues have become more common.\nData Flows To address these challenges, scalable data teams need to develop composable components, separate concerns, and create sustainable and reusable APIs.\nProposed System #In recent years, the industry has established several standards for data processing and storage to ensure scalability, resiliency, and efficient querying. These standards include following:\nSeparating storage, compute, and metadata. Storing data on distributed file systems such as HDFS or object storage like MinIO. Using columnar data formats like ORC and Parquet. In addition, multiple query engines like Spark, Presto, and Hive can be used simultaneously to process stored data through metastore/metadata support.\nHowever, timely insights problems require new standards to achieve solutions, including real-time data ingestion with CDC 1 tools like Debezium 2 and real-time stream processing using Apache Flink, Spark Streaming, and others.\nProblem: Language \u0026amp; Dialect # Dialect: Master SQL There are various languages and dialects for data analytics, like SQL, HiveQL, and KSQL 3, and using conventional programming languages via APIs. However, these languages generate disposable components that require rebuilding functions and readable/writable formats and serializers.\nIn addition, many languages and dialects are not usable by other units and departments within an organization. One optimal solution is to use a master SQL interface such as ZetaSQL 4 and connect interfaces and engine layers to compose concepts.\nProblem: Query Efficiency #Efficient querying is also a significant concern for data platforms, as different query engines like Spark, Presto, or Hive use different components, languages, parsers, analyzers, libraries, and distributed executions, leading to overhead and increased latency.\nUsing separate components that can be used collaboratively in other engines or components can decrease latency, maintenance costs, and management costs.\nSolution: Building Common Components # Connecting Layers via Commmon Components To achieve the desired outcomes, it is important to determine which components are exposed and composable to glue common components together. This should be done while following a manifesto and philosophy that addresses the following concerns:\nHaving a single query/compute engine that can be used for interactive, batch, or streaming analytics. Sharing layers and components such as file formats, serializers, and decoders/encoders. Using common, sustainable, deterministic, and easy-to-use APIs that can be used to interact with other services or units. Joint: Query/Compute Engine #The processing of data can be approached from various perspectives, and there are three types of engine novel data stacks available, namely batch, interactive, and streaming engines.\nBatch Engine #Batch data pipelines typically run on a scheduled basis, such as hourly, daily, or monthly, and utilize available computing resources within the designated pool. These pipelines are not generally designed for sophisticated, real-time processing, but rather for efficiency in terms of resource and time-space requirements.\nThe proposed system design includes batch query engines, as well as other types of engines, that can utilize the same pool of computing resources as an end-to-end layer. This extends beyond just high-level CPU and memory pools, such as those provided by Kubernetes or Mesos 5 resource schedulers. For example, if the data stack utilizes Presto for interactive engine processing, the same SQL query can be executed on the batch engine without requiring any modifications.\nInteractive Engine #Interactive data pipelines or sessions require sub-second query performance, without data copying, and utilize in-memory for data access.\nETL jobs can be run on the interactive engine if they are efficient, but otherwise can be executed on the batch engine. If the data stack employs Spark as the batch engine, the same SQL query can be run on the interactive engine without any modifications. Additionally, all query engines within the stack share a common metastore.\nStreaming Engine #Streaming executions involve distinct concepts that differ from batch and interactive engines, including:\nIngestion techniques from the source, such as checkpointing 6. Processing techniques, such as late or watermark-based processing 7. At-least-once or exactly-once sink methods and semantics 8, as well as the synchronization of metastore partitions. Challenges related to schema evolution 9. These concepts present significant challenges that require careful resolution. Despite these differences, streaming engines share many common components, such as a front-end SQL parser and analyzer, code generation, and coordination and execution layers.\nJoint: Storage #Modern data organizations are composed of several teams, including data scientists, data analysts, data engineers, and machine learning engineers, among others. There are various methods for accessing data, as mentioned in the post. When there is a single interface for accessing data through a Query/Compute Engine Joint, there must be one file or data representation for interacting with storage.\nAn emerging trend in data processing is the use of machine learning, which involves utilizing wider tables with thousands of columns for feature representation and working with higher-dimensional data 10. Another trend enables the seamless file finalization of real-time streaming output. Therefore, the file format used must be capable of supporting these types of data processing.\nJoint: Execution Engine #The most performance-critical component of a data stack is the execution engine, which contains hundreds of computational functions, such as string and numerical operations 11, that handle all processing tasks.\nTypically, each query engine (batch, interactive, or streaming) has its own execution engine or layer, which utilizes several programming languages, such as Java or Scala, and various fault-tolerant mechanisms.\nThe proposed system design features a single execution engine that is integrable by design for every engine, including batch, interactive, and streaming. This design reduces redundant tasks, eliminates code duplication, and improves performance, while also enabling easy adoption of emerging data processing methods, such as machine learning operations. An example of this philosophy can be seen in Velox 12.\nConclusion #Data platforms are constantly evolving and changing, especially with the introduction of artificial intelligence innovations. The following features are essential for a modern data stack:\nComposability between engines and layers. Wide-area caching for each engine and layer. Minimal components in engines and layers. Wide-reaching and sustainable APIs. Minimal code proof. High efficiency and performance with low overhead through strong coordination. Support for machine learning operations by design. Low management and maintenance costs and ease of operation. As technology advances, data stacks will become more refined and sophisticated.\nChange Data Capture\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDebezium\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nksqlDB Overview\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nZetaSQL - Analyzer Framework for SQL\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nApache Mesos\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSpark Streaming Programming Guide\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nStructured Streaming Programming Guide\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nExactly-Once Semantics Are Possible: Hereâ€™s How Kafka Does It\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSchema Evolution and Compatibility\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFeature (machine learning)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\npresto/StringFunctions.java\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHello from Velox\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"9 April 2023","permalink":"/building-common-components-in-data-lakehouse-is-a-necessity/","section":"Writings","summary":"Disclaimer: Any opinions expressed are solely my own.","title":"Building common components in Data Lakehouse is a necessity"},{"content":"","date":null,"permalink":"/tags/data-architecture/","section":"Tags","summary":"","title":"data architecture"},{"content":"","date":null,"permalink":"/tags/data-lakehouse/","section":"Tags","summary":"","title":"data lakehouse"},{"content":"","date":null,"permalink":"/tags/data-platform/","section":"Tags","summary":"","title":"data platform"},{"content":"","date":null,"permalink":"/tags/data-infrastucture/","section":"Tags","summary":"","title":"data infrastucture"},{"content":"","date":null,"permalink":"/tags/kubernetes/","section":"Tags","summary":"","title":"kubernetes"},{"content":" Hacker News discussion Disclaimer: Any opinions expressed are solely my own. Introduction #In brief, Kubernetes is a platform for automating and allocating containers that includes built-in features for healing, scheduling, and deployment practices (such as rollouts and rollbacks), as well as load-balancing and auto-scaling capabilities. Containers are abstracted by the name \u0026ldquo;Pod\u0026rdquo; within Kubernetes.\nKubernetes architecture # Problem #Nowadays, numerous industries face the challenge of collecting and integrating data to solve complex problems. These data-centric problems span from customer service to process automation, and as the number of problems increases, so does the volume of data that needs to be processed. Consequently, the information technology industry is expected to deliver robust, reliable, and scalable data and machine learning solutions more than ever before, to meet the demands of the rapidly growing data volumes.\nProposed System #Almost all data-intensive applications require either batch or real-time data processing. These applications include analytics reports, machine learning tasks, and real-time advertising campaign actions, all of which process large volumes of collected data.\nFor several years, Hadoop has been the preferred choice for batch data processing, and it has primarily used YARN for resource management. However, with the emergence of containerization, Mesos has become a popular resource manager for Hadoop environments. In contrast, real-time processing has always been more complex than batch processing and requires different concepts such as watermarks.\nData applications involve significant computation tasks that must handle large amounts of data at scale. Consequently, these applications require distributed systems and task orchestrations to manage computation devices, processes, threads, containers, pods, or other resources.\nSchedulers are crucial for managing data workloads, whether it is a simple data processing job or a multi-component machine-learning operation flow. Machine learning models typically require an API server, while API servers require load balancing and other microservices concepts.\nFinally, cloud flexibility is a critical consideration for data applications as they must be capable of running on different platforms such as private, public, or hybrid cloud. Data is a valuable asset for companies and governments, and as regulations increase, the importance of data management and processing is only likely to grow. The data race has just begun.\nConclusion #Managing these types of problems and systems can be complex, but Kubernetes has emerged as a solution with several competitive advantages, including:\nResource efficiency: Kubernetes optimizes the use of computing resources and ensures that containers are efficiently allocated to nodes. Seamless scalability on containers or nodes: Kubernetes allows for seamless scaling of containers or nodes based on the needs of the application. Built-in scheduler: Kubernetes has a built-in scheduler that automates the deployment and scaling of containers and manages the allocation of computing resources. Cloud flexibility: Kubernetes provides the flexibility to run applications on different cloud platforms, including private, public, and hybrid clouds. Pushing to apply engineering best practices: Kubernetes encourages the use of best practices in software engineering, such as containerization, microservices, and continuous integration and delivery, to improve the reliability and scalability of applications. ","date":"13 March 2022","permalink":"/why-kubernetes-is-important-for-the-future-of-data-platforms/","section":"Writings","summary":"Hacker News discussion Disclaimer: Any opinions expressed are solely my own.","title":"Why Kubernetes is important for the future of data platforms?"},{"content":"Hi, I\u0026rsquo;m Mert. #Here are some of my technical areas of interest:\nData and artificial intelligence Scalability and high availability Software infrastructure and architecture Want to learn more? Check out my writings.\nPlease feel free to contact me anytime at or .\n","date":"1 January 1970","permalink":"/about/","section":"","summary":"Hi, I\u0026rsquo;m Mert.","title":"About"},{"content":"","date":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"Last updated January 17, 2023\nThis Cookie Policy explains how uses cookies and similar technologies to recognize you when you visit our website at https://mertkavi.com (\u0026ldquo;Website\u0026rdquo;). It explains what these technologies are and why we use them, as well as your rights to control our use of them.\nIn some cases we may use cookies to collect personal information, or that becomes personal information if we combine it with other information.\nWhat are cookies?\nCookies are small data files that are placed on your computer or mobile device when you visit a website. Cookies are widely used by website owners in order to make their websites work, or to work more efficiently, as well as to provide reporting information.\nCookies set by the website owner are called \u0026ldquo;first-party cookies.\u0026rdquo; Cookies set by parties other than the website owner are called \u0026ldquo;third-party cookies.\u0026rdquo; Third-party cookies enable third-party features or functionality to be provided on or through the website (e.g., advertising, interactive content, and analytics). The parties that set these third-party cookies can recognize your computer both when it visits the website in question and also when it visits certain other websites.\nWhy do we use cookies?\nWe use first- and third-party cookies for several reasons. Some cookies are required for technical reasons in order for our Website to operate, and we refer to these as \u0026ldquo;essential\u0026rdquo; or \u0026ldquo;strictly necessary\u0026rdquo; cookies. Other cookies also enable us to track and target the interests of our users to enhance the experience on our Online Properties. Third parties serve cookies through our Website for advertising, analytics, and other purposes. This is described in more detail below.\nHow can I control cookies?\nYou have the right to decide whether to accept or reject cookies. You can exercise your cookie rights by setting your preferences in the Cookie Consent Manager. The Cookie Consent Manager allows you to select which categories of cookies you accept or reject. Essential cookies cannot be rejected as they are strictly necessary to provide you with services.\nThe Cookie Consent Manager can be found in the notification banner and on our website. If you choose to reject cookies, you may still use our website though your access to some functionality and areas of our website may be restricted. You may also set or amend your web browser controls to accept or refuse cookies.\nThe specific types of first- and third-party cookies served through our Website and the purposes they perform are described in the table below (please note that the specific cookies served may vary depending on the specific Online Properties you visit):\nEssential website cookies:\nThese cookies are strictly necessary to provide you with services available through our Website and to use some of its features, such as access to secure areas.\nName:__cf_bm Purpose:Cloudflare places the cookie on end-user devices that access customer sites protected by Bot Management or Bot Fight Mode. Provider:.convertkit.com Service:Cloudflare View Service Privacy Policy\nCountry:United States Type:server_cookie Expires in:30 minutes\nAnalytics and customization cookies: These cookies collect information that is used either in aggregate form to help us understand how our Website is being used or how effective our marketing campaigns are, or to help us customize our Website for you.\nName:_ga Purpose:It records a particular ID used to come up with data about website usage by the user. It is a HTTP cookie that expires after 2 years. Provider:.mertkavi.com Service:Google Analytics View Service Privacy Policy\nCountry:Netherlands Type:http_cookie Expires in:1 year 11 months 29 days Name:ga# Purpose:Used to distinguish individual users by means of designation of a randomly generated number as client identifier, which allows calculation of visits and sessions Provider:.mertkavi.com Service:Google analytics View Service Privacy Policy\nCountry:Netherlands Type:http_cookie Expires in:1 year 11 months 29 days\nUnclassified cookies: These are cookies that have not yet been categorized. We are in the process of classifying these cookies with the help of their providers.\nName:ckid Provider:convertkit.com Country:Netherlands Type:html_local_storage Expires in:persistent\nHow can I control cookies on my browser?\nAs the means by which you can refuse cookies through your web browser controls vary from browser to browser, you should visit your browser\u0026rsquo;s help menu for more information. The following is information about how to manage cookies on the most popular browsers: Chrome Internet Explorer Firefox Safari Edge Opera In addition, most advertising networks offer you a way to opt out of targeted advertising. If you would like to find out more information, please visit: Digital Advertising Alliance Digital Advertising Alliance of Canada European Interactive Digital Advertising Alliance\nWhat about other tracking technologies, like web beacons?\nCookies are not the only way to recognize or track visitors to a website. We may use other, similar technologies from time to time, like web beacons (sometimes called \u0026ldquo;tracking pixels\u0026rdquo; or \u0026ldquo;clear gifs\u0026rdquo;). These are tiny graphics files that contain a unique identifier that enables us to recognize when someone has visited our Website or opened an email including them. This allows us, for example, to monitor the traffic patterns of users from one page within a website to another, to deliver or communicate with cookies, to understand whether you have come to the website from an online advertisement displayed on a third-party website, to improve site performance, and to measure the success of email marketing campaigns. In many instances, these technologies are reliant on cookies to function properly, and so declining cookies will impair their functioning.\nDo you use Flash cookies or Local Shared Objects?\nWebsites may also use so-called \u0026ldquo;Flash Cookies\u0026rdquo; (also known as Local Shared Objects or \u0026ldquo;LSOs\u0026rdquo;) to, among other things, collect and store information about your use of our services, fraud prevention, and for other site operations.\nIf you do not want Flash Cookies stored on your computer, you can adjust the settings of your Flash player to block Flash Cookies storage using the tools contained in the Website Storage Settings Panel. You can also control Flash Cookies by going to the Global Storage Settings Panel and following the instructions (which may include instructions that explain, for example, how to delete existing Flash Cookies (referred to \u0026ldquo;information\u0026rdquo; on the Macromedia site), how to prevent Flash LSOs from being placed on your computer without your being asked, and (for Flash Player 8 and later) how to block Flash Cookies that are not being delivered by the operator of the page you are on at the time).\nPlease note that setting the Flash Player to restrict or limit acceptance of Flash Cookies may reduce or impede the functionality of some Flash applications, including, potentially, Flash applications used in connection with our services or online content.\nDo you serve targeted advertising?\nThird parties may serve cookies on your computer or mobile device to serve advertising through our Website. These companies may use information about your visits to this and other websites in order to provide relevant advertisements about goods and services that you may be interested in. They may also employ technology that is used to measure the effectiveness of advertisements. They can accomplish this by using cookies or web beacons to collect information about your visits to this and other sites in order to provide relevant advertisements about goods and services of potential interest to you. The information collected through this process does not enable us or them to identify your name, contact details, or other details that directly identify you unless you choose to provide these.\nHow often will you update this Cookie Policy?\nWe may update this Cookie Policy from time to time in order to reflect, for example, changes to the cookies we use or for other operational, legal, or regulatory reasons. Please therefore revisit this Cookie Policy regularly to stay informed about our use of cookies and related technologies.\nThe date at the top of this Cookie Policy indicates when it was last updated.\n","date":null,"permalink":"/privacy/","section":"Cookie Policy","summary":"Last updated January 17, 2023","title":"Cookie Policy"}]